{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLM.ipynb","provenance":[],"authorship_tag":"ABX9TyO7JFoFIHJlbjfQIJuRswYg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUKgjlSic5_8","executionInfo":{"status":"ok","timestamp":1620422486138,"user_tz":240,"elapsed":7434,"user":{"displayName":"Mollie Frances Shichman","photoUrl":"","userId":"16231968819224508095"}},"outputId":"74532d9c-8499-4fd9-bdc2-3408c42fbd7f"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 7.9MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 36.2MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 48.0MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2AOvfKDOtQVe","executionInfo":{"status":"ok","timestamp":1620422525391,"user_tz":240,"elapsed":4938,"user":{"displayName":"Mollie Frances Shichman","photoUrl":"","userId":"16231968819224508095"}},"outputId":"2977f20b-975a-44f0-b88a-ffb7b02dd36f"},"source":["!pip install datasets"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting datasets\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n","\u001b[K     |████████████████████████████████| 225kB 8.7MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Collecting fsspec\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n","\u001b[K     |████████████████████████████████| 112kB 13.4MB/s \n","\u001b[?25hCollecting huggingface-hub<0.1.0\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n","Collecting xxhash\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n","\u001b[K     |████████████████████████████████| 245kB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: fsspec, huggingface-hub, xxhash, datasets\n","Successfully installed datasets-1.6.2 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AeRSbvRHc9lZ"},"source":["Copied from https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606},"id":"FV7jOjNUdByo","executionInfo":{"status":"error","timestamp":1620422544516,"user_tz":240,"elapsed":8259,"user":{"displayName":"Mollie Frances Shichman","photoUrl":"","userId":"16231968819224508095"}},"outputId":"05e95e71-4f2c-4ea8-8d44-944b8828c798"},"source":["import logging\n","import math\n","import os\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","from datasets import load_dataset\n","\n","import transformers\n","from transformers import (\n","    CONFIG_MAPPING,\n","    MODEL_FOR_MASKED_LM_MAPPING,\n","    AutoConfig,\n","    AutoModelForMaskedLM,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    HfArgumentParser,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n",")\n","from transformers.trainer_utils import get_last_checkpoint, is_main_process\n","from transformers.utils import check_min_version\n","\n","\n","# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.6.0.dev0\")\n","\n","logger = logging.getLogger(__name__)\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n","    \"\"\"\n","\n","    model_name_or_path: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"The model checkpoint for weights initialization.\"\n","            \"Don't set if you want to train a model from scratch.\"\n","        },\n","    )\n","    model_type: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )\n","    use_fast_tokenizer: bool = field(\n","        default=True,\n","        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n","    )\n","    model_revision: str = field(\n","        default=\"main\",\n","        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n","    )\n","    use_auth_token: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n","            \"with private models).\"\n","        },\n","    )\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","    \"\"\"\n","\n","    dataset_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n","    )\n","    dataset_config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n","    )\n","    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n","    validation_file: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","    validation_split_percentage: Optional[int] = field(\n","        default=5,\n","        metadata={\n","            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n","        },\n","    )\n","    max_seq_length: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated.\"\n","        },\n","    )\n","    preprocessing_num_workers: Optional[int] = field(\n","        default=None,\n","        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n","    )\n","    mlm_probability: float = field(\n","        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n","    )\n","    line_by_line: bool = field(\n","        default=False,\n","        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n","    )\n","    pad_to_max_length: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n","            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n","        },\n","    )\n","    max_train_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","    max_eval_samples: Optional[int] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n","            \"value if set.\"\n","        },\n","    )\n","\n","    def __post_init__(self):\n","        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n","            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n","        else:\n","            if self.train_file is not None:\n","                extension = self.train_file.split(\".\")[-1]\n","                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n","            if self.validation_file is not None:\n","                extension = self.validation_file.split(\".\")[-1]\n","                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n","\n","\n","def main():\n","    # See all possible arguments in src/transformers/training_args.py\n","    # or by passing the --help flag to this script.\n","    # We now keep distinct sets of args, for a cleaner separation of concerns.\n","\n","    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n","    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n","        # If we pass only one argument to the script and it's the path to a json file,\n","        # let's parse it to get our arguments.\n","        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n","    else:\n","        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n","\n","    # Detecting last checkpoint.\n","    last_checkpoint = None\n","    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n","        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n","        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n","            raise ValueError(\n","                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n","                \"Use --overwrite_output_dir to overcome.\"\n","            )\n","        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n","            logger.info(\n","                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n","                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n","            )\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","    )\n","    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n","\n","    # Log on each process the small summary:\n","    logger.warning(\n","        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n","        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n","    )\n","    # Set the verbosity to info of the Transformers logger (on main process only):\n","    if is_main_process(training_args.local_rank):\n","        transformers.utils.logging.set_verbosity_info()\n","        transformers.utils.logging.enable_default_handler()\n","        transformers.utils.logging.enable_explicit_format()\n","    logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","    # Set seed before initializing model.\n","    set_seed(training_args.seed)\n","\n","    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n","    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n","    # (the dataset will be downloaded automatically from the datasets Hub\n","    #\n","    # For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this\n","    # behavior (see below)\n","    #\n","    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n","    # download the dataset.\n","    if data_args.dataset_name is not None:\n","        # Downloading and loading a dataset from the hub.\n","        datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\n","        if \"validation\" not in datasets.keys():\n","            datasets[\"validation\"] = load_dataset(\n","                data_args.dataset_name,\n","                data_args.dataset_config_name,\n","                split=f\"train[:{data_args.validation_split_percentage}%]\",\n","                cache_dir=model_args.cache_dir,\n","            )\n","            datasets[\"train\"] = load_dataset(\n","                data_args.dataset_name,\n","                data_args.dataset_config_name,\n","                split=f\"train[{data_args.validation_split_percentage}%:]\",\n","                cache_dir=model_args.cache_dir,\n","            )\n","    else:\n","        data_files = {}\n","        if data_args.train_file is not None:\n","            data_files[\"train\"] = data_args.train_file\n","        if data_args.validation_file is not None:\n","            data_files[\"validation\"] = data_args.validation_file\n","        extension = data_args.train_file.split(\".\")[-1]\n","        if extension == \"txt\":\n","            extension = \"text\"\n","        datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)\n","    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n","    # https://huggingface.co/docs/datasets/loading_datasets.html.\n","\n","    # Load pretrained model and tokenizer\n","    #\n","    # Distributed training:\n","    # The .from_pretrained methods guarantee that only one local process can concurrently\n","    # download model & vocab.\n","    config_kwargs = {\n","        \"cache_dir\": model_args.cache_dir,\n","        \"revision\": model_args.model_revision,\n","        \"use_auth_token\": True if model_args.use_auth_token else None,\n","    }\n","    if model_args.config_name:\n","        config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n","    elif model_args.model_name_or_path:\n","        config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n","    else:\n","        config = CONFIG_MAPPING[model_args.model_type]()\n","        logger.warning(\"You are instantiating a new config instance from scratch.\")\n","\n","    tokenizer_kwargs = {\n","        \"cache_dir\": model_args.cache_dir,\n","        \"use_fast\": model_args.use_fast_tokenizer,\n","        \"revision\": model_args.model_revision,\n","        \"use_auth_token\": True if model_args.use_auth_token else None,\n","    }\n","    if model_args.tokenizer_name:\n","        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n","    elif model_args.model_name_or_path:\n","        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n","    else:\n","        raise ValueError(\n","            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n","            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n","        )\n","\n","    if model_args.model_name_or_path:\n","        model = AutoModelForMaskedLM.from_pretrained(\n","            model_args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","            config=config,\n","            cache_dir=model_args.cache_dir,\n","            revision=model_args.model_revision,\n","            use_auth_token=True if model_args.use_auth_token else None,\n","        )\n","    else:\n","        logger.info(\"Training new model from scratch\")\n","        model = AutoModelForMaskedLM.from_config(config)\n","\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    # Preprocessing the datasets.\n","    # First we tokenize all the texts.\n","    if training_args.do_train:\n","        column_names = datasets[\"train\"].column_names\n","    else:\n","        column_names = datasets[\"validation\"].column_names\n","    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n","\n","    if data_args.max_seq_length is None:\n","        max_seq_length = tokenizer.model_max_length\n","        if max_seq_length > 1024:\n","            logger.warning(\n","                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n","                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n","            )\n","            max_seq_length = 1024\n","    else:\n","        if data_args.max_seq_length > tokenizer.model_max_length:\n","            logger.warning(\n","                f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n","                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n","            )\n","        max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n","\n","    if data_args.line_by_line:\n","        # When using line_by_line, we just tokenize each nonempty line.\n","        padding = \"max_length\" if data_args.pad_to_max_length else False\n","\n","        def tokenize_function(examples):\n","            # Remove empty lines\n","            examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n","            return tokenizer(\n","                examples[\"text\"],\n","                padding=padding,\n","                truncation=True,\n","                max_length=max_seq_length,\n","                # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n","                # receives the `special_tokens_mask`.\n","                return_special_tokens_mask=True,\n","            )\n","\n","        tokenized_datasets = datasets.map(\n","            tokenize_function,\n","            batched=True,\n","            num_proc=data_args.preprocessing_num_workers,\n","            remove_columns=[text_column_name],\n","            load_from_cache_file=not data_args.overwrite_cache,\n","        )\n","    else:\n","        # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n","        # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n","        # efficient when it receives the `special_tokens_mask`.\n","        def tokenize_function(examples):\n","            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n","\n","        tokenized_datasets = datasets.map(\n","            tokenize_function,\n","            batched=True,\n","            num_proc=data_args.preprocessing_num_workers,\n","            remove_columns=column_names,\n","            load_from_cache_file=not data_args.overwrite_cache,\n","        )\n","\n","        # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n","        # max_seq_length.\n","        def group_texts(examples):\n","            # Concatenate all texts.\n","            concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","            total_length = len(concatenated_examples[list(examples.keys())[0]])\n","            # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n","            # customize this part to your needs.\n","            total_length = (total_length // max_seq_length) * max_seq_length\n","            # Split by chunks of max_len.\n","            result = {\n","                k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n","                for k, t in concatenated_examples.items()\n","            }\n","            return result\n","\n","        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n","        # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n","        # might be slower to preprocess.\n","        #\n","        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n","        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n","\n","        tokenized_datasets = tokenized_datasets.map(\n","            group_texts,\n","            batched=True,\n","            num_proc=data_args.preprocessing_num_workers,\n","            load_from_cache_file=not data_args.overwrite_cache,\n","        )\n","\n","    if training_args.do_train:\n","        if \"train\" not in tokenized_datasets:\n","            raise ValueError(\"--do_train requires a train dataset\")\n","        train_dataset = tokenized_datasets[\"train\"]\n","        if data_args.max_train_samples is not None:\n","            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n","\n","    if training_args.do_eval:\n","        if \"validation\" not in tokenized_datasets:\n","            raise ValueError(\"--do_eval requires a validation dataset\")\n","        eval_dataset = tokenized_datasets[\"validation\"]\n","        if data_args.max_eval_samples is not None:\n","            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n","\n","    # Data collator\n","    # This one will take care of randomly masking the tokens.\n","    pad_to_multiple_of_8 = data_args.line_by_line and training_args.fp16 and not data_args.pad_to_max_length\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm_probability=data_args.mlm_probability,\n","        pad_to_multiple_of=8 if pad_to_multiple_of_8 else None,\n","    )\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset if training_args.do_train else None,\n","        eval_dataset=eval_dataset if training_args.do_eval else None,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","    )\n","\n","    # Training\n","    if training_args.do_train:\n","        checkpoint = None\n","        if training_args.resume_from_checkpoint is not None:\n","            checkpoint = training_args.resume_from_checkpoint\n","        elif last_checkpoint is not None:\n","            checkpoint = last_checkpoint\n","        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","        trainer.save_model()  # Saves the tokenizer too for easy upload\n","        metrics = train_result.metrics\n","\n","        max_train_samples = (\n","            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n","        )\n","        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","        trainer.log_metrics(\"train\", metrics)\n","        trainer.save_metrics(\"train\", metrics)\n","        trainer.save_state()\n","\n","    # Evaluation\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","\n","        metrics = trainer.evaluate()\n","\n","        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n","        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","        perplexity = math.exp(metrics[\"eval_loss\"])\n","        metrics[\"perplexity\"] = perplexity\n","\n","        trainer.log_metrics(\"eval\", metrics)\n","        trainer.save_metrics(\"eval\", metrics)\n","\n","    if training_args.push_to_hub:\n","        trainer.push_to_hub()\n","\n","\n","def _mp_fn(index):\n","    # For xla_spawn (TPUs)\n","    main()\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":4,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-241316bf9be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcheck_min_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"4.6.0.dev0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36mcheck_min_version\u001b[0;34m(min_version)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0merror_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             + (\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;34m\"Check out https://huggingface.co/transformers/examples.html for the examples corresponding to other \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;34m\"versions of HuggingFace Transformers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             )\n","\u001b[0;31mImportError\u001b[0m: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/transformers/installation.html#installing-from-source`), but the version found is 4.5.1.\nCheck out https://huggingface.co/transformers/examples.html for the examples corresponding to other versions of HuggingFace Transformers.","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}